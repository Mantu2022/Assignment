{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a12a883",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1. A company conducted a survey of its employees and found that 70% of the employees use the\n",
    "company's health insurance plan, while 40% of the employees who use the plan are smokers. What is the\n",
    "probability that an employee is a smoker given that he/she uses the health insurance plan?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2162f23c",
   "metadata": {},
   "outputs": [],
   "source": [
    "To find the probability that an employee is a smoker given that he/she uses the health insurance plan, we can use Bayes' theorem.\n",
    "Let:\n",
    "\n",
    "A = event that an employee uses the health insurance plan\n",
    "B = event that an employee is a smoker\n",
    "We are given: P(A)= probability that an employee uses the health insurance plan = 0.70\n",
    "    P(B/A)= probability that an employee is a smoker given that he/she uses the health insurance plan = 0.40\n",
    "    We want to find: P(B/A)= probability that an employee is a smoker given that he/she uses the health insurance plan\n",
    "Using Bayes' theorem: P(B∣A)= P(A/B)*P(B)/P(A) \n",
    "However, we are not directly given P(B), the probability that an employee is a smoker. We can calculate it using the total probability rule:\n",
    "P(B)=P(B∣A)×P(A)+P(B∣¬A)×P(¬A) Where: P(¬A) = probability that an employee does not use the health insurance plan = 1−P(A)\n",
    "P(B∣¬A) = probability that an employee is a smoker given that he/she does not use the health insurance plan    \n",
    "Given that 40% of the employees who use the plan are smokers, we can infer that 60% of the employees who use the plan are non-smokers. Therefore:\n",
    "P(¬A)=1−0.70=0.30\n",
    "P(B∣¬A)=1−0.40=0.60    \n",
    "Now, we can calculate P(B): P(B)=P(B∣A)×P(A)+P(B∣¬A)×P(¬A)\n",
    "P(B)=0.40×0.70+0.60×0.30\n",
    "P(B)=0.28+0.18\n",
    "P(B)=0.46\n",
    "Now, we can use Bayes' theorem to find P(B∣A): \n",
    "P(B∣A)= P(A∣B)×P(B)/P(A)\n",
    "P(B∣A)= 0.40×0.46/0.70\n",
    "P(B∣A)= 0.184/0.70\n",
    "P(B∣A)≈ 0.2629\n",
    "So, the probability that an employee is a smoker given that he/she uses the health insurance plan is approximately 0.2629, or 26.29%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152885f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q2. What is the difference between Bernoulli Naive Bayes and Multinomial Naive Bayes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49eaa0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "The main difference between Bernoulli Naive Bayes and Multinomial Naive Bayes lies in the type of features they are designed to work with and the underlying probability distribution assumed for those features:\n",
    "\n",
    "Bernoulli Naive Bayes:\n",
    "\n",
    "Bernoulli Naive Bayes is suitable for datasets with binary or boolean features, where each feature represents the presence or absence of a particular attribute.\n",
    "It assumes that the features follow a Bernoulli distribution, where each feature is a binary random variable with values 0 or 1.\n",
    "In Bernoulli Naive Bayes, the presence of a feature (value 1) contributes to the likelihood of a class, while the absence of the feature (value 0) does not explicitly contribute to the likelihood.\n",
    "Bernoulli Naive Bayes is commonly used in text classification tasks, such as document categorization or sentiment analysis, where features represent binary indicators of word presence in documents.\n",
    "Multinomial Naive Bayes:\n",
    "\n",
    "Multinomial Naive Bayes is suitable for datasets with categorical or count-based features, where each feature represents the frequency or count of a particular attribute.\n",
    "It assumes that the features follow a multinomial distribution, where each feature is a discrete random variable representing counts of occurrences of different categories.\n",
    "In Multinomial Naive Bayes, the frequencies or counts of features contribute to the likelihood of a class, and each feature value (count) is considered independently.\n",
    "Multinomial Naive Bayes is commonly used in text classification tasks, similar to Bernoulli Naive Bayes, but where features represent counts of word occurrences in documents rather than binary indicators.\n",
    "In summary, Bernoulli Naive Bayes is appropriate for binary features representing presence/absence of attributes, while Multinomial Naive Bayes is suitable for count-based features representing frequencies or counts of attributes. The choice between these two classifiers depends on the nature of the features in the dataset and the assumptions that best align with the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e7d406",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q3. How does Bernoulli Naive Bayes handle missing values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aed59e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Bernoulli Naive Bayes does not handle missing values directly in its standard implementation. When using Bernoulli Naive Bayes, missing values in the dataset need to be handled separately as preprocessing steps before applying the classifier.\n",
    "\n",
    "Here are some common approaches to handle missing values in Bernoulli Naive Bayes:\n",
    "\n",
    "Imputation: Replace missing values with a specific value, such as 0 or 1, to indicate the absence or presence of the corresponding feature. This approach assumes that missing values are equivalent to a certain state of the feature. However, it may introduce bias or inaccuracies if the missing values are not missing completely at random.\n",
    "\n",
    "Deletion: Remove samples or features with missing values from the dataset before applying Bernoulli Naive Bayes. This approach ensures that the classifier is trained only on complete data but may lead to loss of information if the missing values contain valuable information.\n",
    "\n",
    "Model-based imputation: Use a separate model, such as logistic regression or decision trees, to predict missing values based on other features in the dataset. Once the missing values are imputed, Bernoulli Naive Bayes can be applied to the complete dataset. This approach may provide more accurate imputations but requires additional computational resources and introduces additional complexity to the analysis.\n",
    "\n",
    "Treating missing values as a separate category: Treat missing values as a separate category or state of the feature and include it as one of the possible feature values. This approach allows Bernoulli Naive Bayes to directly handle missing values during classification but may require careful consideration of the implications of treating missing values as a distinct category.\n",
    "\n",
    "The choice of approach for handling missing values depends on the specific characteristics of the dataset, the amount of missing data, and the objectives of the analysis. It is important to carefully evaluate the implications of each approach and consider potential biases or inaccuracies introduced by handling missing values in a particular way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d96a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q4. Can Gaussian Naive Bayes be used for multi-class classification?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9fd219d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Yes, Gaussian Naive Bayes can be used for multi-class classification tasks. Gaussian Naive Bayes is an extension of the Naive Bayes algorithm that assumes that the features follow a Gaussian (normal) distribution. It is suitable for continuous or real-valued features.\n",
    "\n",
    "In the case of multi-class classification, where there are more than two classes to predict, Gaussian Naive Bayes can still be applied. The algorithm calculates the conditional probability of each class given the feature values using the Gaussian probability density function. Then, it assigns the class with the highest probability as the predicted class for a given instance.\n",
    "\n",
    "Here's how Gaussian Naive Bayes works for multi-class classification:\n",
    "\n",
    "Model Training:\n",
    "\n",
    "For each class \n",
    " , Gaussian Naive Bayes estimates the mean (\n",
    " ) and variance (\n",
    " ) of each feature's distribution using the training data.\n",
    "It assumes that the features are conditionally independent given the class label, which means that the covariance matrix is diagonal and contains only the variances of each feature.\n",
    "Prediction:\n",
    "\n",
    "Given a new instance with feature values \n",
    "x, Gaussian Naive Bayes calculates the likelihood of the instance belonging to each class \n",
    "  using the Gaussian probability density function with the estimated mean and variance for each feature in each class.\n",
    "It then multiplies the likelihood by the prior probability of each class and normalizes the result to obtain the posterior probability of each class given the feature values \n",
    "x.\n",
    "Finally, it assigns the class with the highest posterior probability as the predicted class for the instance.\n",
    "Gaussian Naive Bayes is a simple and efficient algorithm for multi-class classification, especially when the features are continuous and approximately Gaussian distributed within each class. However, it may not perform well if the features are not well-modeled by a Gaussian distribution or if there are strong dependencies between features. In such cases, other classifiers, such as decision trees or ensemble methods, may be more suitable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4244fc17",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q5. Assignment:\n",
    "Data preparation:\n",
    "Download the \"Spambase Data Set\" from the UCI Machine Learning Repository (https://archive.ics.uci.edu/ml/\n",
    "datasets/Spambase). This dataset contains email messages, where the goal is to predict whether a message\n",
    "is spam or not based on several input features.\n",
    "Implementation:\n",
    "Implement Bernoulli Naive Bayes, Multinomial Naive Bayes, and Gaussian Naive Bayes classifiers using the\n",
    "scikit-learn library in Python. Use 10-fold cross-validation to evaluate the performance of each classifier on the\n",
    "dataset. You should use the default hyperparameters for each classifier.\n",
    "Results:\n",
    "Report the following performance metrics for each classifier:\n",
    "Accuracy\n",
    "Precision\n",
    "Recall\n",
    "F1 score\n",
    "Discussion:\n",
    "Discuss the results you obtained. Which variant of Naive Bayes performed the best? Why do you think that is\n",
    "the case? Are there any limitations of Naive Bayes that you observed?\n",
    "Conclusion:\n",
    "Summarise your findings and provide some suggestions for future work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b4b3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB, GaussianNB\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "# Load the Spambase dataset\n",
    "data = fetch_openml(name='spambase', version=1)\n",
    "\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Initialize classifiers\n",
    "bernoulli_nb = BernoulliNB()\n",
    "multinomial_nb = MultinomialNB()\n",
    "gaussian_nb = GaussianNB()\n",
    "\n",
    "# Perform 10-fold cross-validation for each classifier\n",
    "scoring = ['accuracy', 'precision', 'recall', 'f1']\n",
    "cv_results_bernoulli = cross_validate(bernoulli_nb, X, y, cv=10, scoring=scoring)\n",
    "cv_results_multinomial = cross_validate(multinomial_nb, X, y, cv=10, scoring=scoring)\n",
    "cv_results_gaussian = cross_validate(gaussian_nb, X, y, cv=10, scoring=scoring)\n",
    "\n",
    "# Report performance metrics for each classifier\n",
    "print(\"Bernoulli Naive Bayes:\")\n",
    "print(\"Accuracy:\", cv_results_bernoulli['test_accuracy'].mean())\n",
    "print(\"Precision:\", cv_results_bernoulli['test_precision'].mean())\n",
    "print(\"Recall:\", cv_results_bernoulli['test_recall'].mean())\n",
    "print(\"F1 Score:\", cv_results_bernoulli['test_f1'].mean())\n",
    "print()\n",
    "\n",
    "print(\"Multinomial Naive Bayes:\")\n",
    "print(\"Accuracy:\", cv_results_multinomial['test_accuracy'].mean())\n",
    "print(\"Precision:\", cv_results_multinomial['test_precision'].mean())\n",
    "print(\"Recall:\", cv_results_multinomial['test_recall'].mean())\n",
    "print(\"F1 Score:\", cv_results_multinomial['test_f1'].mean())\n",
    "print()\n",
    "\n",
    "print(\"Gaussian Naive Bayes:\")\n",
    "print(\"Accuracy:\", cv_results_gaussian['test_accuracy'].mean())\n",
    "print(\"Precision:\", cv_results_gaussian['test_precision'].mean())\n",
    "print(\"Recall:\", cv_results_gaussian['test_recall'].mean())\n",
    "print(\"F1 Score:\", cv_results_gaussian['test_f1'].mean())\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7b9107",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
