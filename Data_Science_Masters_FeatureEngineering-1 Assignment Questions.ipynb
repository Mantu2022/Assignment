{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa07453b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1: What are missing values in a dataset? Why is it essential to handle missing values? Name some  algorithms that are not affected by missing values. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb30f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "Missing values in a dataset refer to the absence of data for certain observations or variables. These missing values can occur for various reasons, such as errors during data collection, incomplete surveys, or data corruption. In datasets, missing values are often represented by special symbols like NaN (Not a Number) or NULL.\n",
    "Handling missing values is crucial for several reasons:\n",
    "\n",
    "1. Statistical Accuracy: Missing values can lead to biased or inaccurate statistical analysis and modeling.\n",
    "\n",
    "2. Model Performance: Many machine learning algorithms cannot handle missing values, and their performance may be compromised if not addressed.\n",
    "\n",
    "3. Data Understanding: To gain meaningful insights from data, it's essential to understand the reasons behind missing values and their potential impact on analyses.\n",
    "\n",
    "4. Decision-Making: Incomplete or inaccurate data can lead to faulty decision-making processes.\n",
    "\n",
    "Algorithms that are not affected by missing values:\n",
    "\n",
    "1. Tree-based Algorithms:\n",
    "    Decision Trees\n",
    "    Random Forest\n",
    "    Gradient Boosted Trees (e.g., XGBoost, LightGBM)\n",
    "    Naive Bayes:\n",
    "\n",
    "2. Naive Bayes is not affected by missing values because it works with probabilities and assumes independence between features.\n",
    "3. k-Nearest Neighbors (k-NN):\n",
    "    k-NN can handle missing values by considering only non-missing features when computing distances.\n",
    "    Association Rule Learning:\n",
    "\n",
    "4. Algorithms like Apriori or Eclat for association rule learning are generally not affected by missing values.\n",
    "    Principal Component Analysis (PCA):\n",
    "\n",
    "5.  PCA can be used for dimensionality reduction, and it is not directly affected by missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf84d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q2: List down techniques used to handle missing data.  Give an example of each with python code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f48545",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some common techniques used to handle missing data, along with examples in Python:\n",
    "1. Deletion of Missing Data:\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aaeb1e22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame with rows dropped:\n",
      "     A    B\n",
      "0  1.0  5.0\n",
      "3  4.0  8.0\n",
      "\n",
      "DataFrame with columns dropped:\n",
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: [0, 1, 2, 3]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Example DataFrame with missing values\n",
    "df = pd.DataFrame({'A': [1, 2, None, 4], 'B': [5, None, 7, 8]})\n",
    "\n",
    "# Remove rows with any missing values\n",
    "df_dropped_rows = df.dropna(axis=0)\n",
    "\n",
    "# Remove columns with any missing values\n",
    "df_dropped_columns = df.dropna(axis=1)\n",
    "\n",
    "print(\"DataFrame with rows dropped:\")\n",
    "print(df_dropped_rows)\n",
    "\n",
    "print(\"\\nDataFrame with columns dropped:\")\n",
    "print(df_dropped_columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c66d0cdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame with mean imputation:\n",
      "          A         B\n",
      "0  1.000000  5.000000\n",
      "1  2.000000  6.666667\n",
      "2  2.333333  7.000000\n",
      "3  4.000000  8.000000\n",
      "\n",
      "DataFrame with value imputation:\n",
      "     A    B\n",
      "0  1.0  5.0\n",
      "1  2.0 -1.0\n",
      "2 -1.0  7.0\n",
      "3  4.0  8.0\n"
     ]
    }
   ],
   "source": [
    "# 2. Imputation:\n",
    "import pandas as pd\n",
    "\n",
    "# Example DataFrame with missing values\n",
    "df = pd.DataFrame({'A': [1, 2, None, 4], 'B': [5, None, 7, 8]})\n",
    "\n",
    "# Impute missing values with mean\n",
    "df_imputed_mean = df.fillna(df.mean())\n",
    "\n",
    "# Impute missing values with a specific value\n",
    "df_imputed_value = df.fillna(-1)\n",
    "\n",
    "print(\"DataFrame with mean imputation:\")\n",
    "print(df_imputed_mean)\n",
    "\n",
    "print(\"\\nDataFrame with value imputation:\")\n",
    "print(df_imputed_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e14138db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame with forward fill:\n",
      "     A    B\n",
      "0  1.0  5.0\n",
      "1  2.0  5.0\n",
      "2  2.0  7.0\n",
      "3  4.0  8.0\n"
     ]
    }
   ],
   "source": [
    "# 3. Forward Fill (or Backward Fill):\n",
    "import pandas as pd\n",
    "\n",
    "# Example DataFrame with missing values\n",
    "df = pd.DataFrame({'A': [1, 2, None, 4], 'B': [5, None, 7, 8]})\n",
    "\n",
    "# Forward fill missing values\n",
    "df_forward_filled = df.ffill()\n",
    "\n",
    "print(\"DataFrame with forward fill:\")\n",
    "print(df_forward_filled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d346cda1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame with linear interpolation:\n",
      "     A    B\n",
      "0  1.0  5.0\n",
      "1  2.0  6.0\n",
      "2  3.0  7.0\n",
      "3  4.0  8.0\n"
     ]
    }
   ],
   "source": [
    "# 4. Interpolation:\n",
    "import pandas as pd\n",
    "\n",
    "# Example DataFrame with missing values\n",
    "df = pd.DataFrame({'A': [1, 2, None, 4], 'B': [5, None, 7, 8]})\n",
    "\n",
    "# Linear interpolation\n",
    "df_interpolated = df.interpolate()\n",
    "\n",
    "print(\"DataFrame with linear interpolation:\")\n",
    "print(df_interpolated)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739d875a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q3: Explain the imbalanced data. What will happen if imbalanced data is not handled?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd8aeec",
   "metadata": {},
   "outputs": [],
   "source": [
    "Imbalanced data refers to a situation in a classification problem where the distribution of class labels is not equal. In other words, one class (the minority class) has significantly fewer instances than another class (the majority class). Imbalanced data is common in various real-world scenarios, such as fraud detection, medical diagnosis, and anomaly detection.\n",
    "Consequences of Not Handling Imbalanced Data:\n",
    "\n",
    "1. Biased Model Performance: The model is likely to be biased towards the majority class, leading to suboptimal performance on the minority class.\n",
    "\n",
    "2. Inefficient Learning: The model may not learn meaningful patterns from the minority class due to the dominance of the majority class.\n",
    "\n",
    "3. Limited Insights: Important insights and patterns in the minority class may go unnoticed, especially if they are crucial for decision-making.\n",
    "\n",
    "4. Incorrect Business Decisions: In scenarios where the minority class represents critical events (e.g., fraud, disease), failure to handle imbalanced data may result in incorrect business decisions or missed opportunities.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f43268d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q4: What are Up-sampling and Down-sampling? Explain with an example when up-sampling and down sampling are required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c28f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "Up-sampling and Down-sampling are two techniques used to address imbalanced datasets by adjusting the class distribution. Each technique is applied to either increase or decrease the number of instances in a particular class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367fcdd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Up-sampling:\n",
    "Up-sampling involves increasing the number of instances in the minority class. This can be achieved by either duplicating existing instances or generating synthetic samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df80f6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Example of Up-sampling:\n",
    "Imagine you have a dataset with two classes, A and B. Class B is the minority class. Up-sampling involves creating additional instances of class B to balance the class distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ae05cde0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Up-sampled Data:\n",
      "   Feature Class\n",
      "0        1     A\n",
      "1        2     A\n",
      "2        3     A\n",
      "3        4     A\n",
      "4        5     A\n",
      "7        8     B\n",
      "8        9     B\n",
      "5        6     B\n",
      "7        8     B\n",
      "7        8     B\n"
     ]
    }
   ],
   "source": [
    "# Example using Python and scikit-learn\n",
    "from sklearn.utils import resample\n",
    "import pandas as pd\n",
    "\n",
    "# Sample DataFrame with imbalanced classes\n",
    "data = pd.DataFrame({\n",
    "    'Feature': [1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
    "    'Class': ['A', 'A', 'A', 'A', 'A', 'B', 'B', 'B', 'B']\n",
    "})\n",
    "\n",
    "# Separate majority and minority classes\n",
    "class_A = data[data['Class'] == 'A']\n",
    "class_B = data[data['Class'] == 'B']\n",
    "\n",
    "# Up-sample minority class (Class B)\n",
    "class_B_upsampled = resample(class_B, replace=True, n_samples=len(class_A), random_state=42)\n",
    "\n",
    "# Combine the up-sampled minority class with the majority class\n",
    "upsampled_data = pd.concat([class_A, class_B_upsampled])\n",
    "\n",
    "print(\"Up-sampled Data:\")\n",
    "print(upsampled_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98225cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Down-sampling involves decreasing the number of instances in the majority class. This can be achieved by randomly removing instances from the majority class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f604e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of Down-sampling:\n",
    "# Consider the same dataset with classes A and B. Class A is the majority class. Down-sampling involves randomly removing instances from class A to balance the class distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ce1d3f20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Down-sampled Data:\n",
      "   Feature Class\n",
      "1        2     A\n",
      "4        5     A\n",
      "2        3     A\n",
      "0        1     A\n",
      "5        6     B\n",
      "6        7     B\n",
      "7        8     B\n",
      "8        9     B\n"
     ]
    }
   ],
   "source": [
    "# Example using Python and scikit-learn\n",
    "from sklearn.utils import resample\n",
    "import pandas as pd\n",
    "\n",
    "# Sample DataFrame with imbalanced classes\n",
    "data = pd.DataFrame({\n",
    "    'Feature': [1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
    "    'Class': ['A', 'A', 'A', 'A', 'A', 'B', 'B', 'B', 'B']\n",
    "})\n",
    "\n",
    "# Separate majority and minority classes\n",
    "class_A = data[data['Class'] == 'A']\n",
    "class_B = data[data['Class'] == 'B']\n",
    "\n",
    "# Down-sample majority class (Class A)\n",
    "class_A_downsampled = resample(class_A, replace=False, n_samples=len(class_B), random_state=42)\n",
    "\n",
    "# Combine the down-sampled majority class with the minority class\n",
    "downsampled_data = pd.concat([class_A_downsampled, class_B])\n",
    "\n",
    "print(\"Down-sampled Data:\")\n",
    "print(downsampled_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008d0871",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q5: What is data Augmentation? Explain SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90acaf56",
   "metadata": {},
   "outputs": [],
   "source": [
    "Data Augmentation: Data Augmentation is a technique used to artificially increase the size and diversity of a training dataset by applying various transformations to the existing data. This technique is commonly employed in machine learning, particularly in tasks like image recognition, to prevent overfitting and improve the model's generalization performance.\n",
    "In the context of image data, data augmentation includes operations such as rotation, flipping, scaling, and changes in brightness or contrast. For text data, augmentation may involve introducing synonyms, replacing words, or adding noise. The idea is to create variations of the original data without changing its underlying meaning, allowing the model to learn more robust and generalizable patterns.\n",
    "SMOTE (Synthetic Minority Over-sampling Technique): SMOTE is a specific data augmentation technique designed to address imbalanced datasets, especially in the context of classification problems. Its primary purpose is to overcome the challenge posed by a significant imbalance in the distribution of classes, where one class (minority class) has far fewer instances than another (majority class).\n",
    "\n",
    "Here's how SMOTE works:\n",
    "\n",
    "Identify Minority Class Instances:\n",
    "\n",
    "Identify instances belonging to the minority class in the dataset.\n",
    "Select a Minority Instance:\n",
    "\n",
    "Randomly choose a minority instance from the dataset.\n",
    "Find k Nearest Neighbors:\n",
    "\n",
    "Find the k nearest neighbors of the selected instance within the minority class. The value of k is a user-defined parameter.\n",
    "Generate Synthetic Instances:\n",
    "\n",
    "For each neighbor, create a synthetic instance by interpolating between the selected instance and its neighbor. The synthetic instance is placed at a random point along the line segment connecting the two instances.\n",
    "Repeat for Desired Number of Instances:\n",
    "\n",
    "Repeat the process until the desired number of synthetic instances is generated.\n",
    "Combine Original and Synthetic Instances:\n",
    "\n",
    "Combine the original instances of the minority class with the newly generated synthetic instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f9b8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q6: What are outliers in a dataset? Why is it essential to handle outliers? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c64fc2b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Outliers in a Dataset: Outliers are data points that deviate significantly from the rest of the observations in a dataset. These observations are unusual, rare, or distinct in comparison to the majority of the data. Outliers can occur in one or more dimensions of the data and can have a substantial impact on statistical analyses and machine learning models.\n",
    "\n",
    "Reasons to Handle Outliers:\n",
    "Impact on Descriptive Statistics:\n",
    "\n",
    "Outliers can distort key statistical measures like the mean, median, and standard deviation, leading to a misrepresentation of the central tendency and spread of the data.\n",
    "Skewing Distributions: Outliers can distort the shape of a distribution, making it appear skewed or significantly different from its actual underlying pattern.\n",
    "Affecting Model Assumptions: Many statistical and machine learning models assume a certain distribution or level of homoscedasticity (equal variance). Outliers can violate these assumptions, leading to biased model outcomes.\n",
    "Influence on Regression Models: Outliers can have a disproportionate impact on regression models, pulling the regression line toward them and affecting the model's predictive performance.\n",
    "Data Normalization: Outliers can hinder the effectiveness of normalization techniques, which are commonly used to scale features in machine learning models.\n",
    "Sensitive Models:\n",
    "\n",
    "Some algorithms are sensitive to the presence of outliers, and their performance may degrade if outliers are not addressed.\n",
    "Data Interpretation: Outliers can distort the interpretation of results and conclusions drawn from data analysis, leading to incorrect insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67aa7f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q7: You are working on a project that requires analyzing customer data. However, you notice that some of  the data is missing. What are some techniques you can use to handle the missing data in your analysis?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a2c3fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "Handling missing data is a critical step in the data analysis process. Here are some common techniques we can use to address missing data in your customer dataset:\n",
    "\n",
    "Removal of Missing Data: Complete Case Analysis (CCA): Remove rows with any missing values. This is suitable when the missing data is minimal and removing those instances doesn't significantly impact the analysis.\n",
    "Imputation Techniques: Mean, Median, or Mode Imputation: Replace missing values with the mean, median, or mode of the respective column. This is suitable for numerical data.        \n",
    "Forward Fill (or Backward Fill): Propagate the last valid observation forward or backward in time.        \n",
    "Interpolation: Estimate missing values based on the values of other observations using interpolation methods.\n",
    "Create a Missing Indicator: Create a binary indicator variable to denote whether a value was missing. This can be useful for preserving information about missingness.    \n",
    "Machine Learning-Based Imputation: Use machine learning models (e.g., k-NN, regression) to predict missing values based on other features in the dataset.    \n",
    "Deletion of Columns: If a column has a high proportion of missing values or doesn't contribute significantly to the analysis, consider dropping the entire column.    \n",
    "Multiple Imputation: Generate multiple datasets with different imputed values for missing data and analyze them separately to account for uncertainty.    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6973d9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q8: You are working with a large dataset and find that a small percentage of the data is missing. What are  some strategies you can use to determine if the missing data is missing at random or if there is a pattern  to the missing data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7474968c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Determining whether missing data is missing at random or if there is a pattern to the missing data is crucial for making informed decisions about how to handle it. Here are some strategies you can use to assess the missing data pattern:\n",
    "Visual Inspection: Use visualizations such as heatmaps or missing data matrices to observe the spatial distribution of missing values. If there are noticeable patterns or clusters of missing values, it might suggest non-random missingness.\n",
    "Summary Statistics: Compare summary statistics (mean, median, standard deviation) of variables with missing values to those without missing values. If there are significant differences, it could indicate a non-random pattern.\n",
    "Missing Data Indicator: Create a binary indicator variable that flags whether data is missing. Analyze the relationships between this indicator variable and other variables to identify patterns or correlations.\n",
    "Correlation Analysis: Calculate correlations between missingness in one variable and missingness in other variables. High correlations may suggest patterns in missing data.\n",
    "Missingness Tests: Use statistical tests to assess whether the missingness in a variable is related to other variables. For example, a chi-square test for independence can be used for categorical variables.    \n",
    "Temporal Patterns: Examine if there are temporal patterns in missing data. For example, missing data might occur more frequently during certain time periods or seasons.    \n",
    "Domain Knowledge: Leverage domain knowledge to understand the potential reasons for missingness. If the missingness is related to specific conditions or circumstances, it might not be entirely random.    \n",
    "Machine Learning Models: Train machine learning models to predict missing values based on other features. The model's performance can provide insights into whether there is a discernible pattern in the missing data.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be297907",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q9: Suppose you are working on a medical diagnosis project and find that the majority of patients in the  dataset do not have the condition of interest, while a small percentage do. What are some strategies you  can use to evaluate the performance of your machine learning model on this imbalanced dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25204005",
   "metadata": {},
   "outputs": [],
   "source": [
    "Dealing with imbalanced datasets in a medical diagnosis project, where the majority of patients do not have the condition of interest, requires careful consideration of evaluation strategies. Here are some strategies we can use to evaluate the performance of our machine learning model on an imbalanced dataset:\n",
    "\n",
    "Use Appropriate Evaluation Metrics: Instead of relying solely on accuracy, which can be misleading in imbalanced datasets, use evaluation metrics that provide a more comprehensive view of the model's performance. Common metrics include:\n",
    "Precision: The proportion of true positive predictions among all positive predictions.\n",
    "Recall (Sensitivity): The proportion of true positive predictions among all actual positives.\n",
    "F1-Score: The harmonic mean of precision and recall.\n",
    "Area Under the Receiver Operating Characteristic (ROC-AUC): Measures the trade-off between true positive rate and false positive rate.\n",
    "Confusion Matrix Analysis: Examine the confusion matrix to understand how well the model is performing in terms of true positives, true negatives, false positives, and false negatives. This insight is valuable for understanding where the model may be making errors.\n",
    "Precision-Recall Curve: Plot a precision-recall curve to visualize the trade-off between precision and recall at different decision thresholds. This can help you choose an appropriate threshold based on the specific requirements of the medical diagnosis task.\n",
    "Stratified Cross-Validation: Use stratified cross-validation to ensure that each fold in the cross-validation maintains the same class distribution as the original dataset. This is important for obtaining representative evaluation results.\n",
    "Class Weights: Adjust class weights in your machine learning model to give more importance to the minority class. This helps the model to focus on correctly classifying instances of the minority class.\n",
    "Resampling Techniques: Consider resampling techniques like oversampling the minority class or undersampling the majority class. This helps balance the class distribution in the training data.\n",
    "Ensemble Methods: Use ensemble methods like bagging or boosting, which can be more robust in handling imbalanced datasets.\n",
    "Threshold Adjustment: Experiment with adjusting the classification threshold. This can help find a balance between precision and recall that is suitable for the specific context of the medical diagnosis.\n",
    "Cost-Sensitive Learning: Implement cost-sensitive learning by assigning different misclassification costs to different classes. This encourages the model to prioritize correct predictions for the minority class.\n",
    "Anomaly Detection: Consider treating the medical diagnosis problem as an anomaly detection task, where the minority class is considered the \"anomaly.\" This may involve using models specifically designed for anomaly detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5789456",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q10: When attempting to estimate customer satisfaction for a project, you discover that the dataset is  unbalanced, with the bulk of customers reporting being satisfied. What methods can you employ to  balance the dataset and down-sample the majority class?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7605235",
   "metadata": {},
   "outputs": [],
   "source": [
    "When dealing with an unbalanced dataset, specifically in the context of estimating customer satisfaction, where the majority of customers report being satisfied, you can employ down-sampling techniques to balance the dataset. Down-sampling involves reducing the number of instances in the majority class to match the minority class. Here are some methods you can use:\n",
    "\n",
    "Random Under-sampling: Randomly remove instances from the majority class until the class distribution is balanced.\n",
    "Cluster-Based Under-sampling: Use clustering algorithms to identify clusters within the majority class and down-sample each cluster to balance the dataset.\n",
    "Tomek Links: Identify and remove Tomek links, which are pairs of instances (one from the majority class and one from the minority class) that are nearest neighbors and have different class labels.\n",
    "NearMiss: Use the NearMiss algorithm, which selects instances from the majority class based on their distance to the minority class.\n",
    "Condensed Nearest Neighbors (CNN): Identify and keep only those instances from the majority class that are misclassified by a k-NN classifier.    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f83d8c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q11: You discover that the dataset is unbalanced with a low percentage of occurrences while working on a  project that requires you to estimate the occurrence of a rare event. What methods can you employ to  balance the dataset and up-sample the minority class?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc35e627",
   "metadata": {},
   "outputs": [],
   "source": [
    "When dealing with an imbalanced dataset, particularly in the context of estimating the occurrence of a rare event where the minority class has a low percentage of occurrences, we can employ up-sampling techniques to balance the dataset. Up-sampling involves increasing the number of instances in the minority class to match the majority class. Here are some methods we can use:\n",
    "Random Over-sampling: Randomly duplicate instances from the minority class until the class distribution is balanced.    \n",
    "SMOTE (Synthetic Minority Over-sampling Technique): Generate synthetic instances for the minority class by interpolating between existing instances.    \n",
    "ADASYN (Adaptive Synthetic Sampling): Similar to SMOTE but adjusts the density of synthetic instances based on the local density of minority class instances.    \n",
    "Random Over-sampling with Replacement: Randomly select instances from the minority class with replacement until the class distribution is balanced.    \n",
    "SMOTE-ENN (SMOTE with Edited Nearest Neighbors): Apply SMOTE to generate synthetic instances and then use ENN to remove noisy instances from the majority class.    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
