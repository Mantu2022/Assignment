{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2fbb304",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1. What is Gradient Boosting Regression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c03a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "Gradient Boosting Regression is a powerful machine learning technique used for regression tasks. It is an ensemble learning method that combines the predictions of multiple weak regression models to create a strong regression model. The key idea behind Gradient Boosting Regression is to iteratively train a series of weak regression models, each focusing on the errors (residuals) made by its predecessors, and then combine their predictions in a way that minimizes the overall prediction error.\n",
    "\n",
    "Here's how Gradient Boosting Regression works:\n",
    "\n",
    "Initialization: Gradient Boosting Regression starts with an initial prediction, which is usually a simple estimator such as the mean of the target variable. This initial prediction serves as the starting point for the iterative training process.\n",
    "\n",
    "Sequential Training: In each iteration, a weak regression model (often a decision tree) is trained to predict the residuals (the differences between the actual and predicted values) of the previous model. The weak model is trained on the residuals of the previous predictions, rather than the original target variable. This allows the weak model to focus on the errors made by its predecessors.\n",
    "\n",
    "Gradient Descent Optimization: After training each weak model, the predictions of all previous models are combined to obtain the current prediction. The combined prediction is then compared to the true target values, and the residual errors (the differences between the predicted and true values) are calculated. Gradient descent optimization techniques are used to update the parameters of the weak model in a way that minimizes the residual errors.\n",
    "\n",
    "Iterative Process: The process of training weak models and updating predictions is repeated for a predefined number of iterations or until a stopping criterion is met. Each iteration focuses on reducing the prediction error of the ensemble model by learning from the residuals of the previous models.\n",
    "\n",
    "Final Prediction: The final prediction of the Gradient Boosting Regression model is obtained by combining the predictions of all weak models in the ensemble. The combined prediction represents the ensemble model's estimate of the target variable.\n",
    "\n",
    "Gradient Boosting Regression is known for its ability to capture complex non-linear relationships in the data and produce highly accurate predictions. It is widely used in various regression tasks, including predicting house prices, stock prices, and customer lifetime value. Popular implementations of Gradient Boosting Regression include XGBoost, LightGBM, and CatBoost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3232de53",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q2. Implement a simple gradient boosting algorithm from scratch using Python and NumPy. Use a\n",
    "simple regression problem as an example and train the model on a small dataset. Evaluate the model's\n",
    "performance using metrics such as mean squared error and R-squared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4961c7fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 112732.75714231138\n",
      "R-squared: -79.85688419787752\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Owner\\AppData\\Local\\Temp\\ipykernel_3440\\134224489.py:30: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  predictions = np.sum(estimator.predict(X) for estimator in self.estimators)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class GradientBoostingRegressor:\n",
    "    def __init__(self, n_estimators=100, learning_rate=0.1, max_depth=3):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_depth = max_depth\n",
    "        self.estimators = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Initialize the residuals\n",
    "        residuals = np.copy(y)\n",
    "\n",
    "        for _ in range(self.n_estimators):\n",
    "            # Train a weak learner (decision tree) on the residuals\n",
    "            weak_learner = DecisionTreeRegressor(max_depth=self.max_depth)\n",
    "            weak_learner.fit(X, residuals)\n",
    "            \n",
    "            # Make predictions using the weak learner\n",
    "            predictions = weak_learner.predict(X)\n",
    "            \n",
    "            # Update the residuals by subtracting the predictions\n",
    "            residuals -= self.learning_rate * predictions\n",
    "            \n",
    "            # Add the weak learner to the list of estimators\n",
    "            self.estimators.append(weak_learner)\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Make predictions using the ensemble of weak learners\n",
    "        predictions = np.sum(estimator.predict(X) for estimator in self.estimators)\n",
    "        return predictions\n",
    "\n",
    "# Evaluation metrics\n",
    "def mean_squared_error(y_true, y_pred):\n",
    "    return np.mean((y_true - y_pred) ** 2)\n",
    "\n",
    "def r_squared(y_true, y_pred):\n",
    "    ss_total = np.sum((y_true - np.mean(y_true)) ** 2)\n",
    "    ss_res = np.sum((y_true - y_pred) ** 2)\n",
    "    return 1 - (ss_res / ss_total)\n",
    "\n",
    "# Example usage\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# Generate synthetic dataset\n",
    "X, y = make_regression(n_samples=100, n_features=1, noise=0.1, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the gradient boosting regressor\n",
    "gb_regressor = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3)\n",
    "gb_regressor.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = gb_regressor.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r_squared(y_test, y_pred)\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "print(\"R-squared:\", r2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8815eff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q3. Experiment with different hyperparameters such as learning rate, number of trees, and tree depth to\n",
    "optimise the performance of the model. Use grid search or random search to find the best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb87ce5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'n_estimators': [50, 100, 150],\n",
    "    'max_depth': [2, 3, 4]\n",
    "}\n",
    "\n",
    "# Initialize the gradient boosting regressor\n",
    "gb_regressor = GradientBoostingRegressor()\n",
    "\n",
    "# Perform grid search\n",
    "grid_search = GridSearchCV(gb_regressor, param_grid, cv=5, scoring='neg_mean_squared_error')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "\n",
    "# Train the model with the best hyperparameters\n",
    "best_gb_regressor = GradientBoostingRegressor(**best_params)\n",
    "best_gb_regressor.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = best_gb_regressor.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r_squared(y_test, y_pred)\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "print(\"R-squared:\", r2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169cfc50",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q4. What is a weak learner in Gradient Boosting?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6089e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "In the context of gradient boosting, a weak learner is a simple model that performs slightly better than random guessing on a given task. Specifically, a weak learner is a model that has limited predictive power and is typically less complex than the final ensemble model.\n",
    "\n",
    "In gradient boosting, weak learners are often decision trees with a shallow depth (e.g., decision stumps), linear models with few features, or other simple algorithms. These weak learners are trained sequentially, with each subsequent weak learner focusing on the errors (residuals) made by its predecessors.\n",
    "\n",
    "The concept of using weak learners in gradient boosting is based on the principle of boosting, where the goal is to iteratively improve the performance of the ensemble model by sequentially adding weak learners and combining their predictions. Each weak learner contributes a small amount to the overall prediction, and the final ensemble model is a weighted combination of all weak learners' predictions.\n",
    "\n",
    "The use of weak learners in gradient boosting allows the algorithm to capture complex relationships in the data by iteratively refining the predictions based on the residuals of the previous weak learners. By combining the predictions of multiple weak learners, gradient boosting can create a strong learner that achieves high predictive accuracy on the task at hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8993a92d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q5. What is the intuition behind the Gradient Boosting algorithm?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "984ebd5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "The intuition behind the Gradient Boosting algorithm stems from the idea of sequentially improving the performance of an ensemble model by focusing on the mistakes made by its predecessors. Here's a breakdown of the intuition behind Gradient Boosting:\n",
    "\n",
    "Sequential Improvement: Gradient Boosting works by sequentially training a series of weak learners (often decision trees) on the residuals (errors) of the previous learners. Each weak learner is trained to correct the mistakes made by its predecessors.\n",
    "\n",
    "Gradient Descent Optimization: At each iteration, the algorithm uses gradient descent optimization to update the parameters of the weak learner in a direction that reduces the overall prediction error of the ensemble model. This optimization process ensures that subsequent weak learners focus more on the difficult-to-predict instances.\n",
    "\n",
    "Combining Predictions: After training each weak learner, its predictions are combined with the predictions of the previous learners to produce the final prediction of the ensemble model. The combined prediction represents the ensemble model's estimate of the target variable.\n",
    "\n",
    "Emphasis on Errors: By focusing on the residuals of the previous learners, Gradient Boosting places more emphasis on correcting the errors made by the ensemble model. This iterative process allows the algorithm to gradually improve the model's performance and make better predictions on the training data.\n",
    "\n",
    "Regularization: Gradient Boosting also includes regularization techniques, such as shrinkage (learning rate) and tree pruning, to prevent overfitting and improve the generalization performance of the model.\n",
    "\n",
    "Overall, the intuition behind Gradient Boosting is to iteratively refine the predictions of an ensemble model by sequentially training weak learners on the residuals of the previous learners. This process allows the algorithm to capture complex relationships in the data and create a strong learner that achieves high predictive accuracy on the task at hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "746851cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q6. How does Gradient Boosting algorithm build an ensemble of weak learners?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8155d245",
   "metadata": {},
   "outputs": [],
   "source": [
    "The Gradient Boosting algorithm builds an ensemble of weak learners through a sequential training process. Here's how it works:\n",
    "\n",
    "Initialization: The algorithm starts by initializing the ensemble model with a simple initial prediction. This initial prediction is often the mean of the target variable for regression tasks or a constant value for classification tasks.\n",
    "\n",
    "Sequential Training: The algorithm sequentially trains a series of weak learners, typically decision trees with shallow depths, on the residuals (errors) of the previous learners. Each weak learner focuses on correcting the mistakes made by its predecessors.\n",
    "\n",
    "Gradient Descent Optimization: At each iteration, the algorithm uses gradient descent optimization to update the parameters of the weak learner in a direction that reduces the overall prediction error of the ensemble model. The gradients are computed based on the loss function used in the algorithm (e.g., mean squared error for regression tasks or cross-entropy loss for classification tasks).\n",
    "\n",
    "Combining Predictions: After training each weak learner, its predictions are combined with the predictions of the previous learners using a weighted sum. The weights of the weak learners are determined based on their performance on the training data, with better-performing learners receiving higher weights.\n",
    "\n",
    "Regularization: To prevent overfitting and improve the generalization performance of the model, Gradient Boosting includes regularization techniques such as shrinkage (learning rate) and tree pruning. Shrinkage reduces the contribution of each weak learner to the ensemble, while tree pruning limits the complexity of individual weak learners.\n",
    "\n",
    "Stopping Criterion: The sequential training process continues for a predefined number of iterations (number of trees) or until a stopping criterion is met. Common stopping criteria include reaching a maximum number of iterations, achieving a minimum improvement in performance on a validation set, or when the model converges.\n",
    "\n",
    "By iteratively training weak learners on the residuals of the previous learners and combining their predictions, Gradient Boosting builds an ensemble model that gradually improves its performance and achieves high predictive accuracy on the task at hand. The final ensemble model is a weighted combination of the predictions of all weak learners, with each weak learner focusing on a different aspect of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d01fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q7. What are the steps involved in constructing the mathematical intuition of Gradient Boosting algorithm?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d03ae130",
   "metadata": {},
   "outputs": [],
   "source": [
    "Constructing the mathematical intuition behind the Gradient Boosting algorithm involves understanding the key concepts and mathematical principles that underlie its operation. Here are the steps involved in developing this intuition:\n",
    "\n",
    "Loss Function: Understand the choice of loss function used in the algorithm, such as mean squared error for regression tasks or cross-entropy loss for classification tasks. The loss function quantifies the difference between the predicted values and the true values, providing a measure of the model's performance.\n",
    "\n",
    "Gradient Descent: Gain an understanding of gradient descent optimization, which is used to update the parameters of the weak learners in a direction that minimizes the loss function. Gradient descent computes the gradient of the loss function with respect to the model parameters, indicating the direction of steepest descent.\n",
    "\n",
    "Residuals and Gradient: Recognize that in Gradient Boosting, each weak learner is trained to predict the residuals (errors) of the previous learners. The gradient of the loss function with respect to the residuals provides information about the direction and magnitude of the errors, guiding the training process of subsequent weak learners.\n",
    "\n",
    "Weighted Sum of Predictions: Understand how the predictions of the weak learners are combined using a weighted sum to produce the final prediction of the ensemble model. The weights of the weak learners are determined based on their performance on the training data, with better-performing learners receiving higher weights.\n",
    "\n",
    "Regularization: Learn about regularization techniques such as shrinkage (learning rate) and tree pruning, which are used to prevent overfitting and improve the generalization performance of the model. Shrinkage reduces the contribution of each weak learner to the ensemble, while tree pruning limits the complexity of individual weak learners.\n",
    "\n",
    "Iterative Training Process: Recognize that Gradient Boosting is an iterative training process where weak learners are trained sequentially to improve the performance of the ensemble model. The sequential training process continues until a stopping criterion is met or the model converges.\n",
    "\n",
    "By understanding these key concepts and mathematical principles, you can develop a solid intuition for how the Gradient Boosting algorithm works and how it achieves high predictive accuracy on various machine learning tasks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
